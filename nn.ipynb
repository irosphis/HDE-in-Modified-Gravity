{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.2696367200181884\n",
      "Iteration 100, Loss: 0.23678742738430836\n",
      "Iteration 200, Loss: 0.229031577366213\n",
      "Iteration 300, Loss: 0.2213872436979616\n",
      "Iteration 400, Loss: 0.2135765471289839\n",
      "Iteration 500, Loss: 0.2055708758584916\n",
      "Iteration 600, Loss: 0.19752532426863678\n",
      "Iteration 700, Loss: 0.18968983871630574\n",
      "Iteration 800, Loss: 0.1823128992657243\n",
      "Iteration 900, Loss: 0.17557456034738417\n",
      "Iteration 1000, Loss: 0.16956540776449372\n",
      "Iteration 1100, Loss: 0.1642993554393138\n",
      "Iteration 1200, Loss: 0.15973897416864788\n",
      "Iteration 1300, Loss: 0.15581898991620308\n",
      "Iteration 1400, Loss: 0.15246303792683635\n",
      "Iteration 1500, Loss: 0.14959400232413225\n",
      "Iteration 1600, Loss: 0.1471397270200228\n",
      "Iteration 1700, Loss: 0.14503572991107438\n",
      "Iteration 1800, Loss: 0.14322608468372977\n",
      "Iteration 1900, Loss: 0.1416632475713308\n",
      "Iteration 2000, Loss: 0.140307335150736\n",
      "Iteration 2100, Loss: 0.13912517140519137\n",
      "Iteration 2200, Loss: 0.13808929262007338\n",
      "Iteration 2300, Loss: 0.13717701189792864\n",
      "Iteration 2400, Loss: 0.13636959000797721\n",
      "Iteration 2500, Loss: 0.1356515266655059\n",
      "Iteration 2600, Loss: 0.1350099686651192\n",
      "Iteration 2700, Loss: 0.13443422293928206\n",
      "Iteration 2800, Loss: 0.13391535968105678\n",
      "Iteration 2900, Loss: 0.1334458906420017\n",
      "Iteration 3000, Loss: 0.13301950907036314\n",
      "Iteration 3100, Loss: 0.13263087962582587\n",
      "Iteration 3200, Loss: 0.13227546853832312\n",
      "Iteration 3300, Loss: 0.13194940605278796\n",
      "Iteration 3400, Loss: 0.13164937473567923\n",
      "Iteration 3500, Loss: 0.1313725184985271\n",
      "Iteration 3600, Loss: 0.13111636823728884\n",
      "Iteration 3700, Loss: 0.13087878082552695\n",
      "Iteration 3800, Loss: 0.13065788886833127\n",
      "Iteration 3900, Loss: 0.1304520591542292\n",
      "Iteration 4000, Loss: 0.13025985816159558\n",
      "Iteration 4100, Loss: 0.13008002330717444\n",
      "Iteration 4200, Loss: 0.12991143888586543\n",
      "Iteration 4300, Loss: 0.12975311585777438\n",
      "Iteration 4400, Loss: 0.12960417480240727\n",
      "Iteration 4500, Loss: 0.12946383149005058\n",
      "Iteration 4600, Loss: 0.1293313846240487\n",
      "Iteration 4700, Loss: 0.12920620539049688\n",
      "Iteration 4800, Loss: 0.12908772851823758\n",
      "Iteration 4900, Loss: 0.12897544460541302\n",
      "Iteration 5000, Loss: 0.12886889351189032\n",
      "Iteration 5100, Loss: 0.12876765865173234\n",
      "Iteration 5200, Loss: 0.12867136204822216\n",
      "Iteration 5300, Loss: 0.12857966003703974\n",
      "Iteration 5400, Loss: 0.12849223952208685\n",
      "Iteration 5500, Loss: 0.1284088147039666\n",
      "Iteration 5600, Loss: 0.12832912421390308\n",
      "Iteration 5700, Loss: 0.12825292859644255\n",
      "Iteration 5800, Loss: 0.12818000809303373\n",
      "Iteration 5900, Loss: 0.12811016068586645\n",
      "Iteration 6000, Loss: 0.12804320036741984\n",
      "Iteration 6100, Loss: 0.12797895560625921\n",
      "Iteration 6200, Loss: 0.12791726798388278\n",
      "Iteration 6300, Loss: 0.1278579909810139\n",
      "Iteration 6400, Loss: 0.12780098889476613\n",
      "Iteration 6500, Loss: 0.12774613587067132\n",
      "Iteration 6600, Loss: 0.12769331503574524\n",
      "Iteration 6700, Loss: 0.12764241772060986\n",
      "Iteration 6800, Loss: 0.12759334276028012\n",
      "Iteration 6900, Loss: 0.12754599586457127\n",
      "Iteration 7000, Loss: 0.1275002890502401\n",
      "Iteration 7100, Loss: 0.12745614012797524\n",
      "Iteration 7200, Loss: 0.1274134722382017\n",
      "Iteration 7300, Loss: 0.12737221343041305\n",
      "Iteration 7400, Loss: 0.12733229628137693\n",
      "Iteration 7500, Loss: 0.12729365754812177\n",
      "Iteration 7600, Loss: 0.12725623785209123\n",
      "Iteration 7700, Loss: 0.12721998139127094\n",
      "Iteration 7800, Loss: 0.1271848356774638\n",
      "Iteration 7900, Loss: 0.1271507512962023\n",
      "Iteration 8000, Loss: 0.1271176816870736\n",
      "Iteration 8100, Loss: 0.1270855829424732\n",
      "Iteration 8200, Loss: 0.12705441362301928\n",
      "Iteration 8300, Loss: 0.12702413458805656\n",
      "Iteration 8400, Loss: 0.12699470883983058\n",
      "Iteration 8500, Loss: 0.12696610138008077\n",
      "Iteration 8600, Loss: 0.12693827907791255\n",
      "Iteration 8700, Loss: 0.12691121054793847\n",
      "Iteration 8800, Loss: 0.12688486603777094\n",
      "Iteration 8900, Loss: 0.1268592173240482\n",
      "Iteration 9000, Loss: 0.12683423761625007\n",
      "Iteration 9100, Loss: 0.1268099014676355\n",
      "Iteration 9200, Loss: 0.12678618469269953\n",
      "Iteration 9300, Loss: 0.12676306429059908\n",
      "Iteration 9400, Loss: 0.1267405183740566\n",
      "Iteration 9500, Loss: 0.12671852610329065\n",
      "Iteration 9600, Loss: 0.12669706762456406\n",
      "Iteration 9700, Loss: 0.12667612401298411\n",
      "Iteration 9800, Loss: 0.12665567721921267\n",
      "Iteration 9900, Loss: 0.12663571001978113\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# 初始化网络参数\n",
    "np.random.seed(0)\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 前向传播\n",
    "def forward(X):\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# 反向传播\n",
    "def backward(X, y, z1, a1, z2, a2):\n",
    "    m = y.size\n",
    "    \n",
    "    # 输出层误差\n",
    "    dz2 = a2 - y\n",
    "    dW2 = a1.T.dot(dz2) / m\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "    # 隐藏层误差\n",
    "    dz1 = dz2.dot(W2.T) * sigmoid_derivative(z1)\n",
    "    dW1 = X.T.dot(dz1) / m\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 训练函数\n",
    "def train(X, y, iterations=10000):\n",
    "    global W1, b1, W2, b2\n",
    "    for i in range(iterations):\n",
    "        z1, a1, z2, a2 = forward(X)\n",
    "        dW1, db1, dW2, db2 = backward(X, y, z1, a1, z2, a2)\n",
    "\n",
    "        # 参数更新\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "\n",
    "        # 每100次迭代打印损失\n",
    "        if i % 100 == 0:\n",
    "            loss = np.mean((a2 - y) ** 2)\n",
    "            print(f\"Iteration {i}, Loss: {loss}\")\n",
    "\n",
    "# 示例数据\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "train(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.49802151]\n",
      " [0.50432062]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(X_new, W1, b1, W2, b2):\n",
    "    # 前向传播\n",
    "    z1 = np.dot(X_new, W1) + b1  # 计算第一个隐藏层的线性变换\n",
    "    a1 = sigmoid(z1)                   # 激活函数，比如可以是 ReLU 或 sigmoid\n",
    "    z2 = np.dot(a1, W2) + b2     # 输出层的线性变换\n",
    "    ŷ = sigmoid(z2)                    # 输出层激活函数（可以根据任务调整，例如回归任务不需要激活函数）\n",
    "\n",
    "    return ŷ\n",
    "X_new = np.array([[0, 1], [1, 1]])  # 新输入数据\n",
    "predictions = predict(X_new, W1, b1, W2, b2)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
